{
  "cells": [
    {
      "id": "d3ffe3a2",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv id=\"singlestore-header\" style=\"display: flex; background-color: rgba(124, 195, 235, 0.25); padding: 5px;\"\u003e\n    \u003cdiv id=\"icon-image\" style=\"width: 90px; height: 90px;\"\u003e\n        \u003cimg width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/confluent-logo.png\" /\u003e\n    \u003c/div\u003e\n    \u003cdiv id=\"text\" style=\"padding: 5px; margin-left: 10px;\"\u003e\n        \u003cdiv id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\"\u003eSingleStore Notebooks\u003c/div\u003e\n        \u003ch1 style=\"font-weight: 500; margin: 8px 0 0 4px;\"\u003eIngest data from Confluent Cloud (Kafka)\u003c/h1\u003e\n    \u003c/div\u003e\n\u003c/div\u003e",
      "attachments": {}
    },
    {
      "id": "0a6f9edd",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cimg src=https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/notebooks/confluent-cloud-integration/images/confluent-kafka-integration.png width=\"100%\" /\u003e",
      "attachments": {}
    },
    {
      "id": "5fd6e4ba",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Set Up a Kafka Cluster on Confluent Cloud\n\nBefore initiating the integration process, it is essential to configure a Kafka cluster on Confluent Cloud. Refer to the \u003ca href=\"https://docs.confluent.io/cloud/current/get-started/index.html\"\u003eQuick Start for Confluent Cloud\u003c/a\u003e guide for related information.\n\nOnce the cluster is created, perform the following tasks:\n\n- Create a topic, for example \u003cb\u003e's2-topic'\u003c/b\u003e. On the topic overview page, select \u003cb\u003eSchema \u0026gt; Set a schema \u0026gt; Avro\u003c/b\u003e, and add a new Avro schema. In this guide, the default schema is used.",
      "attachments": {}
    },
    {
      "id": "d56193e9",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cimg src=https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/notebooks/confluent-cloud-integration/images/kafka-value-schema.png width=\"100%\" /\u003e",
      "attachments": {}
    },
    {
      "id": "ede3b8d7",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "- Create API keys. The API key is displayed \u003cb\u003eonly once\u003c/b\u003e. Be sure to copy and securely store the API key.",
      "attachments": {}
    },
    {
      "id": "9ed4b165",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cimg src=https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/notebooks/confluent-cloud-integration/images/confluent-api-key.png width=\"100%\" /\u003e",
      "attachments": {}
    },
    {
      "id": "84749414",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "- On the left navigation pane, select \u003cb\u003eConnectors\u003c/b\u003e and create a sample producer named \u003cb\u003e'datagen'\u003c/b\u003e using the \u003cb\u003eDatagen Source\u003c/b\u003e connector. In the \u003cb\u003eTopic selection\u003c/b\u003e pane, select the \u003cb\u003e's2-topic'\u003c/b\u003e created earlier. In the \u003cb\u003eKafka credentials\u003c/b\u003e pane, select the \u003cb\u003eUse an existing API key\u003c/b\u003e option. Configure the producer to use the same schema as the one in the created topic. Refer to \u003ca href=\"https://docs.confluent.io/cloud/current/get-started/index.html#step-3-create-a-sample-producer\"\u003eStep 3: Create a sample producer\u003c/a\u003e for more information.\n- Launch the \u003cb\u003e'datagen'\u003c/b\u003e producer and verify that the \u003cb\u003e's2-topic'\u003c/b\u003e has new messages.",
      "attachments": {}
    },
    {
      "id": "79e9060a",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Set Up Variables\n\nUse the \u003cb\u003eS2_DATABASE_NAME\u003c/b\u003e, \u003cb\u003eS2_TABLE_NAME\u003c/b\u003e, and \u003cb\u003eS2_PIPELINE_NAME\u003c/b\u003e variables for integration.\n\n### Copy Data from Confluent Cloud\n- Assign the topic name \u003cb\u003e's2-topic'\u003c/b\u003e to the \u003cb\u003eCONFLUENT_KAFKA_TOPIC_NAME\u003c/b\u003e variable.\n- Specify the API key and secret using the \u003cb\u003eCONFLUENT_API_KEY\u003c/b\u003e and \u003cb\u003eCONFLUENT_API_SECRET\u003c/b\u003e variables, respectively.\n\nOn the left navigation pane, select \u003cb\u003eClients\u003c/b\u003e. Select a language (for example Java), and configure the following variables:\n- \u003cb\u003eCONFLUENT_CLUSTER_BOOTSTRAP_SERVER\u003c/b\u003e from \u003cb\u003ebootstrap.servers\u003c/b\u003e\n- \u003cb\u003eCONFLUENT_SCHEMA_REGISTRY_URL\u003c/b\u003e from \u003cb\u003eschema.registry.url\u003c/b\u003e\n\nSelect \u003cb\u003eCreate Schema Registry API key\u003c/b\u003e to create a schema API key and configure the following variables:\n- \u003cb\u003eCONFLUENT_SCHEMA_REGISTRY_KEY\u003c/b\u003e\n- \u003cb\u003eCONFLUENT_SCHEMA_REGISTRY_SECRET\u003c/b\u003e",
      "attachments": {}
    },
    {
      "id": "4fea4101",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "S2_DATABASE_NAME = 'confluent_cloud_integration'\nS2_TABLE_NAME = 'kafka_events'\nS2_PIPELINE_NAME = 'kafka_consumer_pipeline'\nCONFLUENT_KAFKA_TOPIC_NAME = 's2-topic'\nCONFLUENT_KAFKA_CLIENT_ID = 'cwc|001j000000j7k7bAAA|SingleStore'\nCONFLUENT_CLUSTER_BOOTSTRAP_SERVER = 'pkc-xmzwx.europe-central2.gcp.confluent.cloud:9092'\nCONFLUENT_API_KEY = 'EAPEIJZDU5KY26X5'\nCONFLUENT_API_SECRET = '***************************************'\n\nCONFLUENT_SCHEMA_REGISTRY_URL='https://psrc-9zg5y.europe-west3.gcp.confluent.cloud'\nCONFLUENT_SCHEMA_REGISTRY_KEY = '7ALNJUEMWMBIMAQL'\nCONFLUENT_SCHEMA_REGISTRY_SECRET = '***************************************'",
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "75b59d19",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Create a Database",
      "attachments": {}
    },
    {
      "id": "ea3e011e",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nDROP DATABASE IF EXISTS {{S2_DATABASE_NAME}};\nCREATE DATABASE {{S2_DATABASE_NAME}};",
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "cf9309a6",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv class=\"alert alert-block alert-warning\"\u003e    \u003cb class=\"fa fa-solid fa-exclamation-circle\"\u003e\u003c/b\u003e    \u003cdiv\u003e        \u003cp\u003e\u003cb\u003eAction Required\u003c/b\u003e\u003c/p\u003e        \u003cp\u003eBe sure to select the \u003ctt\u003e{{S2_DATABASE_NAME}}\u003c/tt\u003e database from the drop-down list at the top of this notebook.        It updates the \u003ctt\u003econnection_url\u003c/tt\u003e which is used by the \u003ctt\u003e%%sql\u003c/tt\u003e magic command and SQLAlchemy to connect to the selected database.\u003c/p\u003e    \u003c/div\u003e\u003c/div\u003e",
      "attachments": {}
    },
    {
      "id": "623b63ee",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Create a Table Based on the Kafka Avro Schema",
      "attachments": {}
    },
    {
      "id": "5b8cccce",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nDROP PIPELINE IF EXISTS {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}};\nDROP TABLE IF EXISTS {{S2_DATABASE_NAME}}.{{S2_TABLE_NAME}};\nCREATE TABLE IF NOT EXISTS {{S2_DATABASE_NAME}}.{{S2_TABLE_NAME}} (\n`field1` int,\n`field2` double,\n`field3` text\n);",
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "f1dabcd9",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Create a Kafka Pipeline",
      "attachments": {}
    },
    {
      "id": "0aed93cd",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv class=\"alert alert-block alert-info\"\u003e    \u003cb class=\"fa fa-solid fa-exclamation-circle\"\u003e\u003c/b\u003e    \u003cdiv\u003e        \u003cp\u003e\u003cb\u003eNotes\u003c/b\u003e\u003c/p\u003e        \u003cul\u003e\u003cli\u003e\u003cp\u003e All Kafka configurations in the pipeline, such as \u003ctt\u003e'client.id'\u003c/tt\u003e, are supported since version \u003ctt\u003e8.1.35\u003c/tt\u003e.\u003c/p\u003e\u003c/li\u003e \u003cli\u003e\u003cp\u003e The schema registry mapping section should be updated according to your schema registry in the  \u003ci\u003e'table column name'\u003c/i\u003e  \u003c-  \u003ci\u003e'schema registry field name'\u003c/i\u003e format. \u003c/p\u003e\u003c/li\u003e  \u003c/ul\u003e    \u003c/div\u003e\u003c/div\u003e",
      "attachments": {}
    },
    {
      "id": "58c471f0",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nDROP PIPELINE IF EXISTS {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}};\nCREATE PIPELINE {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}}\nAS LOAD DATA KAFKA '{{CONFLUENT_CLUSTER_BOOTSTRAP_SERVER}}/{{CONFLUENT_KAFKA_TOPIC_NAME}}'\nCONFIG '{ \\\"client.id\\\": \\\"{{CONFLUENT_KAFKA_CLIENT_ID}}\\\",\\n         \\\"sasl.username\\\": \\\"{{CONFLUENT_API_KEY}}\\\",\\n         \\\"sasl.mechanism\\\": \\\"PLAIN\\\",\\n         \\\"security.protocol\\\": \\\"SASL_SSL\\\",\\n         \\\"ssl.ca.location\\\": \\\"/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\\\",\\n \\\"schema.registry.username\\\": \\\"{{CONFLUENT_SCHEMA_REGISTRY_KEY}}\\\"}'\nCREDENTIALS '{\\\"sasl.password\\\": \\\"{{CONFLUENT_API_SECRET}}\\\",\\n \\\"schema.registry.password\\\": \\\"{{CONFLUENT_SCHEMA_REGISTRY_SECRET}}\\\"}'\nBATCH_INTERVAL 20\nDISABLE OFFSETS METADATA GC\nINTO TABLE {{S2_TABLE_NAME}}\nFORMAT AVRO\nSCHEMA REGISTRY '{{CONFLUENT_SCHEMA_REGISTRY_URL}}'\n(\nfield1  \u003c-  my_field1,\nfield2  \u003c-  my_field2,\nfield3  \u003c-  my_field3\n);",
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "1ea816f8",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Test the Created Pipeline",
      "attachments": {}
    },
    {
      "id": "95565993",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nTEST PIPELINE  {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}} LIMIT 1;",
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "ca3bb824",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Start the Pipeline",
      "attachments": {}
    },
    {
      "id": "31f8bc3f",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nSTART PIPELINE {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}};",
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "d17352a4",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### Stop the Pipeline",
      "attachments": {}
    },
    {
      "id": "93d4b5c0",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nSTOP PIPELINE {{S2_DATABASE_NAME}}.{{S2_PIPELINE_NAME}};",
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "d6e9fda0",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "### View Consumed Events",
      "attachments": {}
    },
    {
      "id": "d4d40067",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\nSELECT * FROM {{S2_DATABASE_NAME}}.{{S2_TABLE_NAME}};",
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "d6a5e9d7",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cimg src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/\u003e\u003c/div\u003e",
      "attachments": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimeType": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "singlestore_connection": {
      "connectionID": "",
      "defaultDatabase": ""
    },
    "singlestore_cell_default_language": "python",
    "singlestore_row_limit": 300
  },
  "nbformat": 4,
  "nbformat_minor": 5
}