{
  "cells": [
    {
      "id": "7aac61ac",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv id=\"singlestore-header\" style=\"display: flex; background-color: rgba(124, 195, 235, 0.25); padding: 5px;\"\u003e\n    \u003cdiv id=\"icon-image\" style=\"width: 90px; height: 90px;\"\u003e\n        \u003cimg width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/book-open-cover.png\" /\u003e\n    \u003c/div\u003e\n    \u003cdiv id=\"text\" style=\"padding: 5px; margin-left: 10px;\"\u003e\n        \u003cdiv id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\"\u003eSingleStore Notebooks\u003c/div\u003e\n        \u003ch1 style=\"font-weight: 500; margin: 8px 0 0 4px;\"\u003eSearching all of Wikipedia\u003c/h1\u003e\n    \u003c/div\u003e\n\u003c/div\u003e",
      "attachments": {}
    },
    {
      "id": "31deb491",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv class=\"alert alert-block alert-info\"\u003e\n    \u003cb class=\"fa fa-solid fa-info-circle\"\u003e\u003c/b\u003e\n    \u003cdiv\u003e\n        \u003cp\u003e\u003cb\u003eNote\u003c/b\u003e\u003c/p\u003e\n        \u003cp\u003eThis tutorial is meant for Standard \u0026 Premium Workspaces. You can't run this with a Free Starter Workspace due to restrictions on Storage. Create a Workspace using +group in the left nav \u0026 select Standard for this notebook. Gallery notebooks tagged with \"Starter\" are suitable to run on a Free Starter Workspace \u003c/p\u003e\n    \u003c/div\u003e\n\u003c/div\u003e",
      "attachments": {}
    },
    {
      "id": "b80ec63d",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "In this notebook, we've embarked on the task to implement semantic search and Retrieval-Augmented Generation (RAG) across Wikipedia's extensive database, using SingleStore's indexed ANN search capabilities.\n\nWe have focused on a subset of Wikipedia, scraping 1,800 video game articles to obtain real OpenAI embeddings for about 40,000 paragraphs, supplemented by 10 million mock vectors for a scaled-up simulation. We then stored these vectors in a SingleStore database, applying different indexing options to enhance search efficiency. We show remarkable improvements in query response times, dropping to sub 100 milliseconds with indexing.\n\nWe also integrated this system with a RAG-based chat, where you can ask and retrieve contextual answers based on the video game information in the database. Additionally, I've shared the technical details and resources, including our Python code and data links, in [this GitHub repository](https://github.com/rohitbhamidi/singlestore-indexed-ann) and an AWS S3 bucket (S3 URI: s3://wikipedia-video-game-data/video-game-embeddings(1).csv).",
      "attachments": {}
    },
    {
      "id": "e30504b2",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 0: Creating your workspace and database\n\nFor the purposes of this demo, we are only generating 10 million vectors to search through. By our estimations, Wikipedia contains around 160 million paragraphs as a whole. A quick heuristic for your workspace sizing:\n\n- 160 million vectors can be handled by an S-32 Workspace\n- 20 million vectors can be handled by an S-4 Workspace\n\nYou can now extrapolate the workspace size you require from the number of vectors you want to generate!",
      "attachments": {}
    },
    {
      "id": "59d7247b",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\ndrop database if exists video_game_wikipedia;\ncreate database video_game_wikipedia;\nuse video_game_wikipedia;\n\n-- create our table (note the usage of the vector data type)\ncreate table vecs(\n    id bigint(20),\n    url text default null,\n    paragraph text default null,\n    v vector(1536) not null,\n    shard key(id),\n    key(id) using hash,\n    fulltext (paragraph)\n);",
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "0a0390ac",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 1: Generating the Mock Vectors\n\nA `vecs` table is created with fields for ID, URL, paragraph text, and the embedding vector.\n\nThen, we define several functions:\n- `randbetween`: generates random floats in a specified range\n- `gen_vector`: creates a vector of given length filled with random values\n- `normalize`: adjusts a vector to unit length\n- `norm1536`: normalizes a vector of dimension 1536\n- `nrandv1536`: generates a normalized, random vector of dimension 1536\n\nFinally, we populate `vecs` with 10,000,000 rows of mock vectors.",
      "attachments": {}
    },
    {
      "id": "1258b171",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- generates random floats in a specified range\ncreate or replace function randbetween(a float, b float) returns float\nas\nbegin\n  return (rand()*(b - a) + a);\nend ;",
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "c098908b",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- set the sql_mode so that the following function returns the expected result\nset sql_mode = pipes_as_concat;\n\n-- creates a vector of given length filled with random values\ncreate or replace function gen_vector(length int) returns text as\ndeclare s text = \"[\";\nbegin\n  if length \u003c 2 then\n    raise user_exception(\"length too short: \" || length);\n  end if;\n\n  for i in 1..length-1 loop\n    s = s || randbetween(-1,1) || \",\" ;\n  end loop;\n  s = s || randbetween(-1,1) || \"]\";\n  return s;\nend;",
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "0b707241",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- adjusts a vector to unit length\ncreate or replace function normalize(v blob) returns blob as\ndeclare\n  squares blob = vector_mul(v,v);\n  length float = sqrt(vector_elements_sum(squares));\nbegin\n  return scalar_vector_mul(1/length, v);\nend;",
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "614f85d0",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- normalizes a vector of dimension 1536\ncreate or replace function norm1536(v vector(1536)) returns vector(1536) as\nbegin\n  return normalize(v) :\u003e vector(1536);\nend;",
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "f5e7a62d",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- generates a normalized, random vector of dimension 1536\ncreate or replace function nrandv1536() returns vector(1536) as\nbegin\n  return norm1536(gen_vector(1536));\nend;",
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "90571219",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- procedure to populate `vecs` with `num_rows` vectors\ncreate or replace procedure insert_vectors(num_rows bigint) as\ndeclare c int;\nbegin\n  select count(*) into c from vecs;\n  loop\n    insert into vecs (id, v)\n    select id + (select max(id) from vecs), nrandv1536()\n    from vecs\n    where id \u003c= 128 * 1024; /* chunk size 128K so we can see progress */\n    select count(*) into c from vecs;\n    if c \u003e= num_rows then\n      exit;\n    end if;\n  end loop;\nend;",
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "81b39e22",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- run the procedure to populate `vecs` with 10,000,000 vectors\n-- this will take around 20 min\ninsert into vecs (id, v) values (1, nrandv1536());\ncall insert_vectors(10000000);",
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "0f2f6417",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "As a quick aside, if you want to generate the full 160 million vectors, you simply have to change the `num_rows` to 160,000,000: `call insert_vectors(160000000);`",
      "attachments": {}
    },
    {
      "id": "adce04a3",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 2: Getting the Wikipedia video game data\n\nWe will use a SingleStore pipeline named `wiki_pipeline` to import data from an S3 bucket into `vecs`. The pipeline is configured to load data from a CSV file located at `s3://wikipedia-video-game-data/video-game-embeddings(1).csv`. Since the S3 bucket is open, the credentials section is left empty.",
      "attachments": {}
    },
    {
      "id": "02e85cce",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- since the bucket is open, you can leave the credentials clause as it is\ncreate or replace pipeline `wiki_pipeline` as\nload data S3 's3://wikipedia-video-game-data/video-game-embeddings(1).csv'\nconfig '{\"region\":\"us-west-1\"}'\ncredentials '{\"aws_access_key_id\": \"\",\n            \"aws_secret_access_key\": \"\"}'\nskip duplicate key errors\ninto table `vecs`\nformat csv\nfields terminated by ','\nenclosed by '\"'\nlines terminated by '\\r\\n';",
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "46b2136c",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- start the pipeline!\nstart pipeline `wiki_pipeline`;",
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "4f9b0875",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n-- monitor the pipeline!\nselect DATABASE_NAME, PIPELINE_NAME, BATCH_ID, BATCH_STATE, START_TIME, ROWS_STREAMED, ROWS_PER_SEC\nfrom information_schema.PIPELINES_BATCHES_SUMMARY\norder by BATCH_ID;",
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "636ec475",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 3: Building the vector indexes\n\nNow, we have all the data in our table `vecs`. Let's go ahead and build our vector index. SingleStore gives us many options for our index with many tunable parameters. We will stick with the IVF indexes with default parameters.",
      "attachments": {}
    },
    {
      "id": "40f3072a",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nalter table vecs add vector index auto (v) INDEX_OPTIONS '{\"index_type\":\"AUTO\"}';",
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "3a626544",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nalter table vecs add vector index ivf_flat (v) INDEX_OPTIONS '{\"index_type\":\"IVF_FLAT\"}';",
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "bc8a3b33",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nalter table vecs add vector index ivf_pq (v) INDEX_OPTIONS '{\"index_type\":\"IVF_PQ\"}';",
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "5ade8ae2",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 4: Testing our indexes\n\nNow that we have indexed our 10M vector dataset, let us now run some queries to test index performance!\n\nWe have chosen a test vector whose paragraph is about Nintendo's Rad Racer video game. We will compare the performance of an exact K-nearest neighbor search to the searches with our ANN indexes.\n\nAs we will see, we get an order of magnitude improvement when using an index in comparison to the exact KNN search!",
      "attachments": {}
    },
    {
      "id": "9f40900b",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nset @qv = (select v from vecs where id = 1125899906845489);\n\n-- NO INDEX: exact kNN search\nselect paragraph, v \u003c*\u003e @qv as sim\nfrom vecs\norder by sim use index () desc\nlimit 5;",
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "51f21307",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nset @qv = (select v from vecs where id = 1125899906845489);\n\n-- AUTO index\nselect paragraph, v \u003c*\u003e @qv as sim\nfrom vecs\norder by sim use index (auto) desc\nlimit 5;",
      "outputs": [],
      "execution_count": 16
    },
    {
      "id": "9e9a8fa9",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nset @qv = (select v from vecs where id = 1125899906845489);\n\n-- IVF_FLAT\nselect paragraph, v \u003c*\u003e @qv as sim\nfrom vecs\norder by sim use index (ivf_flat) desc\nlimit 5;",
      "outputs": [],
      "execution_count": 17
    },
    {
      "id": "d46484c5",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\nset @qv = (select v from vecs where id = 1125899906845489);\n\n-- IVF_PQ\nselect paragraph, v \u003c*\u003e @qv as sim\nfrom vecs\norder by sim use index (ivf_pq) desc\nlimit 5;",
      "outputs": [],
      "execution_count": 18
    },
    {
      "id": "4b1113d0",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 5: Hybrid Search in SingleStore\n\nLet us now see how we can implement a \"hybrid search\" in SingleStore! This is going to be a query that combines two powerful tools: a fulltext search and a semantic search!",
      "attachments": {}
    },
    {
      "id": "5a402fee",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "%%sql\n\n-- set the query vector\nset @v_mario = (select v\n                from vecs where url = \"https://en.wikipedia.org/wiki/Super_Mario_Kart\"\n                order by id\n                limit 1);\n\n-- building the hybrid search\nwith fts as(\n    select id, paragraph, match (paragraph) against ('Mario Kart') as score\n    from vecs\n    where match (paragraph) against ('Mario Kart')\n    order by score desc\n    limit 200\n),\nvs as (\n    select id, paragraph, v \u003c*\u003e @v_mario as score\n    from vecs\n    order by score use index (auto) desc\n    limit 200\n)\nselect vs.id,\n    vs.paragraph,\n    .3 * ifnull(fts.score, 0) + .7 * vs.score as hybrid_score,\n    vs.score as vec_score,\n    ifnull(fts.score, 0) as ft_score\nfrom fts full outer join vs\n    on fts.id = vs.id\norder by hybrid_score desc\nlimit 5;",
      "outputs": [],
      "execution_count": 19
    },
    {
      "id": "c6f7ede2",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "## Part 6: Chatting with the Video Game data!\n\n- `search_wiki_page` Function:\n  - Conducts semantic search in a database.\n  - Uses embeddings from `get_embedding` for query input.\n  - Finds top 'k' paragraphs in `vecs` table, ranked by similarity to query.\n  - Measures and prints search execution time.\n\n- `ask_wiki_page` Function:\n  - Utilizes results from `search_wiki_page` for chatbot input.\n  - Generates a query for an OpenAI GPT model-based chatbot.",
      "attachments": {}
    },
    {
      "id": "27551786",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "!pip3 install openai --quiet",
      "outputs": [],
      "execution_count": 20
    },
    {
      "id": "de331a08",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "import sqlalchemy as sa\nfrom openai import OpenAI\nimport getpass\nimport os\nimport time\nimport json",
      "outputs": [],
      "execution_count": 21
    },
    {
      "id": "36038718",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "# OpenAI connection\nOPENAI_API_KEY = os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\nclient = OpenAI()\nEMBEDDING_MODEL = 'text-embedding-ada-002'\nGPT_MODEL = 'gpt-3.5-turbo'\n\n# SingleStore connection\nengine = sa.create_engine(connection_url)\nconnection = engine.connect()",
      "outputs": [],
      "execution_count": 22
    },
    {
      "id": "eb5b430d",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "def get_embedding(text, model=EMBEDDING_MODEL):\n    '''Generates the OpenAI embedding from an input `text`.'''\n    if isinstance(text, str):\n        response = client.embeddings.create(input=[text], model=model)\n        return json.dumps(response.data[0].embedding)",
      "outputs": [],
      "execution_count": 23
    },
    {
      "id": "abcbb35b",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "def search_wiki_page(query, limit=5):\n    '''Returns a df of the top k matches to the query ordered by similarity.'''\n    query_embedding_vec = get_embedding(query)\n    statement = sa.text(\n        f'''select paragraph, v \u003c*\u003e :query_embedding :\u003e vector(1536) AS similarity\n        from vecs\n        order by similarity use index (auto) desc\n        limit :limit;'''\n    )\n    print(\"Searching for matches...\")\n    start_time = time.time()\n    results = connection.execute(statement, {\"query_embedding\": query_embedding_vec, \"limit\": limit})\n    end_time = time.time()\n    execution_time = end_time - start_time\n    print(f\"Search complete in {execution_time} seconds.\")\n    results_as_dict = results.fetchall()\n    return results_as_dict",
      "outputs": [],
      "execution_count": 24
    },
    {
      "id": "852878d7",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "def ask_wiki_page(query, limit=5, temp=0.0):\n    '''Uses RAG to answer a question from the wiki page'''\n    results = search_wiki_page(query, limit)\n    print(\"Asking Chatbot...\")\n    prompt = f'''Excerpt from the conversation history:\n        {results}\n        Question: {query}\n\n        Based on the conversation history, try to provide the most accurate answer to the question.\n        Consider the details mentioned in the conversation history to formulate a response that is as\n        helpful and precise as possible. please provide links to WIKIPEDIA ARTICLES TO LOOK AT FOR MORE INFORMATION.\n\n        Most importantly, IF THE INFORMATION IS NOT PRESENT IN THE CONVERSATION HISTORY, DO NOT MAKE UP AN ANSWER.'''\n    response = client.chat.completions.create(\n        model=GPT_MODEL,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant who is answering questions about an article.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=temp\n    )\n    response_message = response.choices[0].message.content\n    return response_message",
      "outputs": [],
      "execution_count": 25
    },
    {
      "id": "ac0fc0f3",
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "language": "python"
      },
      "source": "query = input('Ask me a question about video games!')\nask_wiki_page(query)",
      "outputs": [],
      "execution_count": 26
    },
    {
      "id": "d53fccb8",
      "cell_type": "markdown",
      "metadata": {
        "execution": {}
      },
      "source": "\u003cdiv id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cimg src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/\u003e\u003c/div\u003e",
      "attachments": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimeType": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "jupyterlab": {
      "notebooks": {
        "version_major": 6,
        "version_minor": 4
      }
    },
    "singlestore_connection": {
      "connectionID": "",
      "defaultDatabase": ""
    },
    "singlestore_cell_default_language": "python",
    "singlestore_row_limit": 300
  },
  "nbformat": 4,
  "nbformat_minor": 5
}