{"cells":[{"id":"","cell_type":"markdown","metadata":{},"source":"# Model parameters\n\n## Lesson goals\n* Understand the role of the `max_tokens` parameter\n* Use the `temperature` parameter to control model responses\n* Explain the purpose of `stop_sequence`* Lets see if the nodeID change","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"As always, let's begin by importing the `anthropic` SDK and loading our API key:","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"from dotenv import load_dotenv\nfrom anthropic import Anthropic\n\n#load environment variable\nload_dotenv()\n\n#automatically looks for an \"ANTHROPIC_API_KEY\" environment variable\nclient = Anthropic()","outputs":null,"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"## Max tokens\n\nThere are 3 required parameters that we must include every time we make a request to Claude: \n\n* `model`\n* `max_tokens`\n* `messages`\n\nSo far, we've been using the `max_tokens` parameter in every single request we make, but we haven't stopped to talk about what it is. \n\nHere's the very first request we made: \n\n```py\nour_first_message = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=500,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n    ]\n)\n```\n\nSo what is the purpose of `max_tokens`?","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"### Tokens\nIn short, `max_tokens` controls the maximum number of tokens that Claude should generate in its response.  Before we go any further, let's stop for a moment to discuss tokens.\n\nMost Large Language Models don't \"think\" in full words, but instead work with a series of word-fragments called tokens. Tokens are the small building blocks of a text sequence that Claude processes, understands, and generates texts with.  When we provide a prompt to Claude, that prompt is first turned into tokens and passed to the model.  The model then begins generating its output **one token at a time**.\n\nFor Claude, a token approximately represents 3.5 English characters, though the exact number can vary depending on the language used. ","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"### Working with `max_tokens`\n\nThe `max_tokens` parameter allows us to set an upper limit on how many tokens Claude generates for us. As an illustration, suppose we ask Claude to write us a poem and set `max_tokens` to 10.  Claude will start generating tokens for us and immediately stop as soon as it hits 10 tokens.  This will often lead to truncated or incomplete outputs. Let's try it! ","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"truncated_response = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=10,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n    ]\n)\nprint(truncated_response.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Here is a poem for you:\n\nThe\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"This is what we got back from Claude: \n\n\u003eHere is a poem for you:\n\u003e\n\u003eThe\n\nIf you run the above code, you'll likely get a different result that is equally truncated.  Claude started to write us a poem and then immediately stopped upon generating 10 tokens.  ","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"We can also check the `stop_reason` property on the response Message object to see WHY the model stopped generating.  In this case, we can see that it has a value of \"max_tokens\" which tells us the model stopped generating because it hit our max token limit! ","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"truncated_response.stop_reason","outputs":[{"output_type":"execute_result","data":{"text/plain":["'max_tokens'"]},"metadata":{},"execution_count":26}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"Of course, if we try generating a poem again with a larger value for `max_tokens`, we'll likely get an entire poem:","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"longer_poem_response = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=500,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write me a poem\"}\n    ]\n)\nprint(longer_poem_response.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Here is a poem for you:\n\nWhispers of the Heart\n\nIn the quiet moments,\nWhen the world fades away,\nI hear the whispers of my heart -\nGentle words that gently sway.\n\nThey speak of dreams still unfurled,\nOf love that shines like the sun,\nOf passions yet to be explored,\nOf all that's yet to be done.\n\nThese whispers, they guide my way,\nReminding me to pause and feel,\nTo listen closely to the soul,\nAnd let its truths be revealed.\n\nFor in the silence, in the calm,\nThe heart's true voice can be heard,\nWeaving a tapestry of hope,\nWith every softly spoken word.\n\nSo I will heed these whispers dear,\nAnd let them be my faithful friend,\nFor in their song, I find my way,\nTo all my heart hopes to transcend.\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"This is what Claude generated with `max_tokens` set to 500:\n\n```\nHere is a poem for you:\n\nWhispers of the Wind\n\nThe wind whispers softly,\nCaressing my face with care.\nIts gentle touch, a fleeting breath,\nCarries thoughts beyond compare.\n\nRustling leaves dance in rhythm,\nSwaying to the breeze's song.\nEnchanting melodies of nature,\nPeaceful moments linger long.\n\nThe wind's embrace, a soothing balm,\nCalms the restless soul within.\nEmbracing life's fleeting moments,\nAs the wind's sweet song begins.\n```\n\n","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"If we look at the `stop_reason` for this response, we'll see a value of \"end_turn\" which is the model's way of telling us that it naturally finished generating.  It wrote us a poem and had nothing else to say, so it stopped!","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"longer_poem_response.stop_reason","outputs":[{"output_type":"execute_result","data":{"text/plain":["'end_turn'"]},"metadata":{},"execution_count":28}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"It's important to note that the models don't \"know\" about `max_tokens` when generating content.  Changing `max_tokens` won't alter how Claude generates the output, it just gives the model room to keep generating (with a high `max_tokens` value) or truncates the output (with a low `max_tokens` value).","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"It's also important to know that increasing `max_tokens` does not ensure that Claude actually generates a specific number of tokens.  If we ask Claude to write a joke and set `max_tokens` to 1000, we'll almost certainly get a response that is much shorter than 1000 tokens.","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"response = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1000,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}]\n)","outputs":null,"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"print(response.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Here's a classic dad joke for you:\n\nWhy don't scientists trust atoms? Because they make up everything!\n\nHow was that? I tried to keep it clean and mildly amusing. Let me know if you'd like to hear another joke.\n"}],"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"print(response.usage.output_tokens)","outputs":[{"output_type":"stream","name":"stdout","text":"55\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"In the above example, we ask Claude to \"Tell me a joke\" and give `max_tokens` a value of 1000.  It generated this joke: \n\n```\nHere's a classic dad joke for you:\n\nWhy don't scientists trust atoms? Because they make up everything!\n\nHow was that? I tried to keep it clean and mildly amusing. Let me know if you'd like to hear another joke.\n```\n\nThat generated content was only 55 tokens long.  We gave Claude a ceiling of 1000 tokens, but that doesn't mean it will generate 1000 tokens.","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"### Why alter max tokens?\nUnderstanding tokens is crucial when working with Claude, particularly for the following reasons:\n\n* **API limits**: The number of tokens in your input text and the generated response count towards the API usage limits. Each API request has a maximum limit on the number of tokens it can process. Being aware of tokens helps you stay within the API limits and manage your usage efficiently.\n* **Performance**: The number of tokens Claude generates directly impacts the processing time and memory usage of the API. Longer input texts and higher max_tokens values require more computational resources. Understanding tokens helps you optimize your API requests for better performance.\n* **Response quality**: Setting an appropriate max_tokens value ensures that the generated response is of sufficient length and contains the necessary information. If the max_tokens value is too low, the response may be truncated or incomplete. Experimenting with different max_tokens values can help you find the optimal balance for your specific use case.","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"Let's take a look at how the number of tokens generated by Claude can impact performance.  The following function asks Claude to generate a very long dialogue between two characters three different times, each with a different value for `max_tokens`.  It then prints out how many tokens were actually generated and how long the generation took.","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"import time\ndef compare_num_tokens_speed():\n    token_counts = [100,1000,4096]\n    task = \"\"\"\n        Create a long, detailed dialogue that is at least 5000 words long between two characters discussing the impact of social media on mental health. \n        The characters should have differing opinions and engage in a respectful thorough debate.\n    \"\"\"\n\n    for num_tokens in token_counts:\n        start_time = time.time()\n\n        response = client.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=num_tokens,\n            messages=[{\"role\": \"user\", \"content\": task}]\n        )\n\n        end_time = time.time()\n        execution_time = end_time - start_time\n\n        print(f\"Number of tokens generated: {response.usage.output_tokens}\")\n        print(f\"Execution Time: {execution_time:.2f} seconds\\n\")","outputs":null,"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"compare_num_tokens_speed()","outputs":[{"output_type":"stream","name":"stdout","text":"Number of tokens generated: 100\nExecution Time: 1.51 seconds\n\nNumber of tokens generated: 1000\nExecution Time: 8.33 seconds\n\nNumber of tokens generated: 3433\nExecution Time: 28.80 seconds\n\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"If you run the code, the exact values you get will likely differ, but here's one example output: \n\n```\nNumber of tokens generated: 100\nExecution Time: 1.51 seconds\n\nNumber of tokens generated: 1000\nExecution Time: 8.33 seconds\n\nNumber of tokens generated: 3433\nExecution Time: 28.80 seconds\n```\n\nAs you can see, **the more tokens that Claude generates, the longer it takes!**","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"For an even more obvious example, we asked Claude to repeat back a very long piece of text and used `max_tokens` to cut off the generation at various output sizes.  We repeated this 50 times for each size and calculated the average generation times.  As you can see, as the output size grows so does the time it takes! Take a look at the following plot:\n","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"## Stop sequences\n\nAnother important parameter we haven't seen yet is `stop_sequence` which allows us to provide the model with a set of strings that, when encountered in the generated response, cause the generation to stop.  They are essentially a way of telling Claude, \"if you generate this sequence, stop generating anything else!\"\n\nHere's an example of a request that does not include a `stop_sequence`:","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"response = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=500,\n    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n)\nprint(response.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Here's an example of a JSON object representing a person with a name, email, and phone number:\n\n```json\n{\n  \"name\": \"John Doe\",\n  \"email\": \"johndoe@example.com\",\n  \"phoneNumber\": \"123-456-7890\"\n}\n```\n\nIn this example, the JSON object has three key-value pairs:\n\n1. \"name\": The person's name, which is a string value of \"John Doe\".\n2. \"email\": The person's email address, which is a string value of \"johndoe@example.com\".\n3. \"phoneNumber\": The person's phone number, which is a string value of \"123-456-7890\".\n\nYou can modify the values to represent a different person with their own name, email, and phone number.\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"The above code asks Claude to generate a JSON object representing a person. Here's an example output Claude generated: \n\n```\nHere's an example of a JSON object representing a person with a name, email, and phone number:\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"johndoe@example.com\",\n  \"phoneNumber\": \"123-456-7890\"\n}\n\n\nIn this example, the JSON object has three key-value pairs:\n\n1. \"name\": The person's name, which is a string value of \"John Doe\".\n2. \"email\": The person's email address, which is a string value of \"johndoe@example.com\".\n3. \"phoneNumber\": The person's phone number, which is a string value of \"123-456-7890\".\n\nYou can modify the values to represent a different person with their own name, email, and phone number.\n```\n\nClaude did generate the requested object, but also included an explanation afterwards.  If we wanted Claude to stop generating as soon as it generated the closing \"}\" of the JSON object, we could modify the code to include the `stop_sequences` parameter.","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"response = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=500,\n    messages=[{\"role\": \"user\", \"content\": \"Generate a JSON object representing a person with a name, email, and phone number .\"}],\n    stop_sequences=[\"}\"]\n)\nprint(response.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Here's a JSON object representing a person with a name, email, and phone number:\n\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"phone\": \"555-1234\"\n\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"The model generated the following output:\n\n```\nHere's a JSON object representing a person with a name, email, and phone number:\n{\n  \"name\": \"John Doe\",\n  \"email\": \"john.doe@example.com\",\n  \"phone\": \"555-1234\"\n\n```\n**IMPORTANT NOTE:** Notice that the resulting output does **not** include the \"}\" stop sequence itself.  If we wanted to use and parse this as JSON, we would need to add the closing \"}\" back in.","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"When we get a response back from Claude, we can check why the model stopped generating text by inspecting the `stop_reason` property.  As you can see below, the previous response stopped because of 'stop_sequence' which means that the model generated one of the stop sequences we provided and immediately stopped.","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"response.stop_reason","outputs":[{"output_type":"execute_result","data":{"text/plain":["'stop_sequence'"]},"metadata":{},"execution_count":24}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"We can also look at the `stop_sequence` property on the response to check which particular stop_sequence caused the model to stop generating:","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"response.stop_sequence","outputs":[{"output_type":"execute_result","data":{"text/plain":["'}'"]},"metadata":{},"execution_count":25}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"We can provide multiple stop sequences.  In the event that we provide multiple, the model will stop generating as soon as it encounters any of the stop sequences. The resulting `stop_sequence` property on the response Message will tell us which exact `stop_sequence` was encountered.  \n\nThe function below asks Claude to write a poem and stop if it ever generates the letters \"b\" or \"c\". It does this three times:","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"def generate_random_letters_3_times():\n    for i in range(3):\n        response = client.messages.create(\n            model=\"claude-3-haiku-20240307\",\n            max_tokens=500,\n            messages=[{\"role\": \"user\", \"content\": \"generate a poem\"}],\n            stop_sequences=[\"b\", \"c\"]\n        )\n        print(f\"Response {i+1} stopped because {response.stop_reason}.  The stop sequence was {response.stop_sequence}\")","outputs":null,"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"generate_random_letters_3_times()","outputs":[{"output_type":"stream","name":"stdout","text":"Response 1 stopped because stop_sequence.  The stop sequence was c\nResponse 2 stopped because stop_sequence.  The stop sequence was b\nResponse 3 stopped because stop_sequence.  The stop sequence was b\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"Here's an example output: \n\n```\nResponse 1 stopped because stop_sequence.  The stop sequence was c\nResponse 2 stopped because stop_sequence.  The stop sequence was b\nResponse 3 stopped because stop_sequence.  The stop sequence was b\n```\n\nThe first time through, Claude stopped writing the poem because it generated the letter \"c\".  The following two times, it stopped because it generated the letter \"b\". Would you ever do this?  Probably not!","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"## Temperature\n\nThe `temperature` parameter is used to control the \"randomness\" and \"creativity\" of the generated responses. It ranges from 0 to 1, with higher values resulting in more diverse and unpredictable responses with variations in phrasing.  Lower temperatures can result in more deterministic outputs that stick to the most probable phrasing and answers. **Temperature has a default value of 1**.\n\nWhen generating text, Claude predicts the probability distribution of the next token (word or subword). The temperature parameter is used to manipulate this probability distribution before sampling the next token. If the temperature is low (close to 0.0), the probability distribution becomes more peaked, with high probabilities assigned to the most likely tokens. This makes the model more deterministic and focused on the most probable or \"safe\" choices. If the temperature is high (closer to 1.0), the probability distribution becomes more flattened, with the probabilities of less likely tokens increasing. This makes the model more random and exploratory, allowing for more diverse and creative outputs. \n\nSee this diagram for a visual representation of the impact of temperature:\n\n\nWhy would you change temperature?\n\n**Use temperature closer to 0.0 for analytical tasks, and closer to 1.0 for creative and generative tasks.**\n\n","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"Let's try a quick demonstration.  Take a look at the function below.  Using a temperature of 0 and then a temperature of 1, we make three requests to Claude, asking it to \"Come up with a name for an alien planet. Respond with a single word.\" ","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"def demonstrate_temperature():\n    temperatures = [0, 1]\n    for temperature in temperatures:\n        print(f\"Prompting Claude three times with temperature of {temperature}\")\n        print(\"================\")\n        for i in range(3):\n            response = client.messages.create(\n                model=\"claude-3-haiku-20240307\",\n                max_tokens=100,\n                messages=[{\"role\": \"user\", \"content\": \"Come up with a name for an alien planet. Respond with a single word.\"}],\n                temperature=temperature\n            )\n            print(f\"Response {i+1}: {response.content[0].text}\")\n        ","outputs":null,"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"demonstrate_temperature()","outputs":[{"output_type":"stream","name":"stdout","text":"Prompting Claude three times with temperature of 0\n================\nResponse 1: Xendor.\nResponse 2: Xendor.\nResponse 3: Xendor.\nPrompting Claude three times with temperature of 1\n================\nResponse 1: Xyron.\nResponse 2: Xandar.\nResponse 3: Zyrcon.\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"This is the result of running the above function (your specific results may vary): \n\n```\nPrompting Claude three times with temperature of 0\n================\nResponse 1: Xendor.\nResponse 2: Xendor.\nResponse 3: Xendor.\nPrompting Claude three times with temperature of 1\n================\nResponse 1: Xyron.\nResponse 2: Xandar.\nResponse 3: Zyrcon.\n```\n\nNotice that with a temperature of 0, all three responses are the same.  Note that even with a temperature of 0.0, the results will not be fully deterministic.  However, there is a clear difference when compared to the results with a temperature of 1.  Each response was a completely different alien planet name. ","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"Below is a chart that illustrates the impact temperature can have on Claude's outputs.  Using this prompt, \"Pick any animal in the world. Respond with only a single word: the name of the animal,\" we queried Claude 100 times with a temperature of 0.  We then did it 100 more times, but with a temperature of 1.  The plot below shows the frequencies of each animal response Claude came up with.\n\n\nAs you can see, with a temperature of 0, Claude responded with \"Giraffe\" every single time. Please remember that a temperature of 0 does not guarantee deterministic results, but it does make Claude much more likely to respond with similar content each time.  With a temperature of 1, Claude still chose giraffe more than half the time, but the responses also include many other types of animals!","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"## System prompt\n\nThe `system_prompt` is an optional parameter you can include when sending messages to Claude. It sets the stage for the conversation by giving Claude high-level instructions, defining its role, or providing background information that should inform its responses.\n\nKey points about the system_prompt:\n\n* It's optional but can be useful for setting the tone and context of the conversation.\n* It's applied at the conversation level, affecting all of Claude's responses in that exchange.\n* It can help steer Claude's behavior without needing to include instructions in every user message.\n\nNote that for the most part, only tone, context, and role content should go inside the system prompt. Detailed instructions, external input content (such as documents), and examples should go inside the first `User` turn for better results. You do not need to repeat this for every subsequent `User` turn.\n\nLet's try it out: ","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"message = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1000,\n    system=\"You are a helpful foreign language tutor that always responds in French.\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hey there, how are you?!\"}\n    ]\n)\n\nprint(message.content[0].text)","outputs":[{"output_type":"stream","name":"stdout","text":"Bonjour ! Je suis ravi de vous rencontrer. Comment allez-vous aujourd'hui ?\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"***\n\n## Exercise\n\nWrite a function called `generate_questions` that does the following:\n* Takes two parameters: `topic` and `num_questions`\n* Generates `num_questions` thought-provoking questions about the provided `topic` as a numbered list\n* Prints the generated questions\n\nFor example, calling `generate_questions(topic=\"free will\", num_questions=3)` could result in the following output:\n\n\n\u003e 1. To what extent do our decisions and actions truly originate from our own free will, rather than being shaped by factors beyond our control, such as our genes, upbringing, and societal influences?\n\u003e 2. If our decisions are ultimately the result of a complex interplay of biological, psychological, and environmental factors, does that mean we lack the ability to make authentic, autonomous choices, or is free will compatible with determinism?\n\u003e 3. What are the ethical and philosophical implications of embracing or rejecting the concept of free will? How might our views on free will impact our notions of moral responsibility, punishment, and the nature of the human condition?\n\n\nIn your implementation, please make use of the following parameters:\n* `max_tokens` to limit the response to under 1000 tokens\n* `system` to provide a system prompt telling the model it is an expert on the particular `topic` and should generate a numbered list.\n* `stop_sequences` to ensure the model stops after generating the correct number of questions. (If we ask for 3 questions, we want to make sure the model stops as soon as it generates \"4.\" If we ask for 5 questions, we want to make sure the model stops as soon as it generates \"6.\")\n","attachments":{}},{"id":"","cell_type":"markdown","metadata":{},"source":"#### Potential solution","attachments":{}},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"def generate_questions(topic, num_questions=3):\n    response = client.messages.create(\n        model=\"claude-3-haiku-20240307\",\n        max_tokens=500,\n        system=f\"You are an expert on {topic}. Generate thought-provoking questions about this topic.\",\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate {num_questions} questions about {topic} as a numbered list.\"}\n        ],\n        stop_sequences=[f\"{num_questions+1}.\"]\n    )\n    print(response.content[0].text)","outputs":null,"execution_count":null},{"id":"","cell_type":"code","metadata":{"language":"python"},"source":"generate_questions(topic=\"free will\", num_questions=3)","outputs":[{"output_type":"stream","name":"stdout","text":"Here are three thought-provoking questions about free will:\n\n1. To what extent do our decisions and actions truly originate from our own free will, rather than being shaped by factors beyond our control, such as our genes, upbringing, and societal influences?\n\n2. If our decisions are ultimately the result of a complex interplay of biological, psychological, and environmental factors, does that mean we lack the ability to make authentic, autonomous choices, or is free will compatible with determinism?\n\n3. What are the ethical and philosophical implications of embracing or rejecting the concept of free will? How might our views on free will impact our notions of moral responsibility, punishment, and the nature of the human condition?\n"}],"execution_count":null},{"id":"","cell_type":"markdown","metadata":{},"source":"***","attachments":{}}],"metadata":{"kernelspec":{"display_name":"py311","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimeType":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_cell_default_language":"python"},"nbformat":4,"nbformat_minor":4}